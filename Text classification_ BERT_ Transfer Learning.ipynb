{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7849aea-8499-4844-804a-c7b6ad123a33",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/boscochanam/text-classification-bert-transfer-learning/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c02ef67-9957-4448-ba9e-78b8856761d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ce90f6-ee62-4786-aaed-2d11d4ca4102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497d7b3-cd06-4afa-bd0d-71d7fe585a07",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97aff42-a7d0-412c-9570-7a1c2a1f8d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords and tokenizer from NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize stopwords and Porter Stemmer\n",
    "stop_words = stopwords.words('english')\n",
    "prt = nltk.stem.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b248253c-52c9-4d3a-9911-c1b5d5ca5698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Convert to lowercase and remove punctuation\n",
    "    tokens_pun_lower = [i.lower() for i in tokens if i.isalnum()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens_stop = [i for i in tokens_pun_lower if i not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    terms = [prt.stem(i) for i in tokens_stop]\n",
    "    \n",
    "    # Return the processed text\n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb54f76c-e156-404b-8688-64869ab80882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0                  Fears for T N pension after talks   \n",
      "1  The Race is On: Second Private Team Sets Launc...   \n",
      "2      Ky. Company Wins Grant to Study Peptides (AP)   \n",
      "3      Prediction Unit Helps Forecast Wildfires (AP)   \n",
      "4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
      "\n",
      "                                         Description  \\\n",
      "0  Unions representing workers at Turner   Newall...   \n",
      "1  SPACE.com - TORONTO, Canada -- A second\\team o...   \n",
      "2  AP - A company founded by a chemistry research...   \n",
      "3  AP - It's barely dawn when Mike Fitzpatrick st...   \n",
      "4  AP - Southern California's smog-fighting agenc...   \n",
      "\n",
      "                               Processed_Description  Class Index  \n",
      "0  union repres worker turner newal say talk stri...            3  \n",
      "1  toronto canada rocket compet 36 10 million ans...            4  \n",
      "2  ap compani found chemistri research univers lo...            4  \n",
      "3  ap bare dawn mike fitzpatrick start shift blur...            4  \n",
      "4  ap southern california agenc went emiss bovin ...            4  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV into a DataFrame\n",
    "import os \n",
    "\n",
    "os.chdir(r'E:\\Python code\\ag-news-classification-dataset')\n",
    "\n",
    "csv_path = 'test.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Apply the preprocess function to the 'Description' column\n",
    "df['Processed_Description'] = df['Description'].apply(preprocess)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df[['Title', 'Description', 'Processed_Description', 'Class Index']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877c6b2-a826-4c5a-851d-798066c0193c",
   "metadata": {},
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20593ccd-f7be-41c2-942b-2c812acebdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673be003f03444dbb8e427381060a45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69725cd5ca484f939014828c9f6ac8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1ecc6e68ab49819ccfe6edc7266646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f65d55b8b74ee8a5713367b73218fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f29aa707-10b3-4a3e-b049-4776c2bf8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenizer.encode_plus(\n",
    "    df['Processed_Description'].iloc[0], \n",
    "    max_length=256, \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    add_special_tokens=True,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf3a6dc-7df8-4bfa-856d-541dd5b14a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 256), dtype=int32, numpy=\n",
       "array([[  101,  3779,  1231,  1643,  4894,  7589,  1885,  1200,  1207,\n",
       "         1348,  1474,  2037, 18178,  6486,  3016,  7672,  1200,   182,\n",
       "         8032,  4654,   102,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb592e03-bd24-494c-824e-28c7aabc8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_ids = np.zeros((len(df), 256))\n",
    "X_attn_masks = np.zeros((len(df), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "868f904e-10b3-4c71-b053-482a8e23b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(df['Processed_Description'])):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=256, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8925531b-12b4-47ec-8c82-56e8a1ac7c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72de9535beca40b0997f58583c50600a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebebce0c-42c9-4f9b-a4c5-c3828f7ec1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7600, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.zeros((len(df), 10))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fdc7fd9-9ff5-4a62-b54e-3d8e7c370e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Processed_Description</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "      <td>union repres worker turner newal say talk stri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "      <td>toronto canada rocket compet 36 10 million ans...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "      <td>ap compani found chemistri research univers lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "      <td>ap bare dawn mike fitzpatrick start shift blur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "      <td>ap southern california agenc went emiss bovin ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>1</td>\n",
       "      <td>Around the world</td>\n",
       "      <td>Ukrainian presidential candidate Viktor Yushch...</td>\n",
       "      <td>ukrainian presidenti candid viktor yushchenko ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>2</td>\n",
       "      <td>Void is filled with Clement</td>\n",
       "      <td>With the supply of attractive pitching options...</td>\n",
       "      <td>suppli attract pitch option dwindl daili lost ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>2</td>\n",
       "      <td>Martinez leaves bitter</td>\n",
       "      <td>Like Roger Clemens did almost exactly eight ye...</td>\n",
       "      <td>like roger clemen almost exactli eight year ea...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>3</td>\n",
       "      <td>5 of arthritis patients in Singapore take Bext...</td>\n",
       "      <td>SINGAPORE : Doctors in the United States have ...</td>\n",
       "      <td>singapor doctor unit state warn painkil bextra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>3</td>\n",
       "      <td>EBay gets into rentals</td>\n",
       "      <td>EBay plans to buy the apartment and home renta...</td>\n",
       "      <td>ebay plan buy apart home rental servic 415 mil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class Index                                              Title  \\\n",
       "0               3                  Fears for T N pension after talks   \n",
       "1               4  The Race is On: Second Private Team Sets Launc...   \n",
       "2               4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "3               4      Prediction Unit Helps Forecast Wildfires (AP)   \n",
       "4               4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
       "...           ...                                                ...   \n",
       "7595            1                                   Around the world   \n",
       "7596            2                        Void is filled with Clement   \n",
       "7597            2                             Martinez leaves bitter   \n",
       "7598            3  5 of arthritis patients in Singapore take Bext...   \n",
       "7599            3                             EBay gets into rentals   \n",
       "\n",
       "                                            Description  \\\n",
       "0     Unions representing workers at Turner   Newall...   \n",
       "1     SPACE.com - TORONTO, Canada -- A second\\team o...   \n",
       "2     AP - A company founded by a chemistry research...   \n",
       "3     AP - It's barely dawn when Mike Fitzpatrick st...   \n",
       "4     AP - Southern California's smog-fighting agenc...   \n",
       "...                                                 ...   \n",
       "7595  Ukrainian presidential candidate Viktor Yushch...   \n",
       "7596  With the supply of attractive pitching options...   \n",
       "7597  Like Roger Clemens did almost exactly eight ye...   \n",
       "7598  SINGAPORE : Doctors in the United States have ...   \n",
       "7599  EBay plans to buy the apartment and home renta...   \n",
       "\n",
       "                                  Processed_Description  Class  \n",
       "0     union repres worker turner newal say talk stri...      0  \n",
       "1     toronto canada rocket compet 36 10 million ans...      1  \n",
       "2     ap compani found chemistri research univers lo...      1  \n",
       "3     ap bare dawn mike fitzpatrick start shift blur...      1  \n",
       "4     ap southern california agenc went emiss bovin ...      1  \n",
       "...                                                 ...    ...  \n",
       "7595  ukrainian presidenti candid viktor yushchenko ...      3  \n",
       "7596  suppli attract pitch option dwindl daili lost ...      2  \n",
       "7597  like roger clemen almost exactli eight year ea...      2  \n",
       "7598  singapor doctor unit state warn painkil bextra...      0  \n",
       "7599  ebay plan buy apart home rental servic 415 mil...      0  \n",
       "\n",
       "[7600 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'] = pd.factorize(df['Class Index'])[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58e91dcc-726a-46b1-bbf5-3000f098e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.arange(len(df)), df['Class Index'].values.astype(int)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f2f7f30-189c-4422-bb42-70c8a0062c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(10,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a data pipeline using tensorflow dataset utility, creates batches of data for easy loading...\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
    "dataset.take(1) # one sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e9d1401-14cd-438c-ba83-690a9008ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelDatasetMapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "209264bd-8591-488e-bede-f2608bf7d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(ModelDatasetMapFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db75f18b-d7ea-457f-9693-ad30608ca91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(10,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46d39e56-f42c-451f-80ef-196ccf1ee788",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "487bfa9b-8042-4ef7-b9d4-99e194764291",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.8\n",
    "train_size = int((len(df)//16)*p) # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ec478d0-7c14-4989-952e-60579cd5a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9601053-af86-4e3a-985c-87c734b820cb",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2af6e445-b3b5-4171-b9e8-b842fa9a6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "153db41c-3346-4cfe-a7df-b1224da3c394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7cdb676e634c45b22aa9f2c536f22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2392c9fa-9b92-4595-bfbb-d8561d57acae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 256)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)      TFBaseModelOutputWithPooli   1083102   ['input_ids[0][0]',           \n",
      "                             ngAndCrossAttentions(last_   72         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, 256, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " intermediate_layer (Dense)  (None, 512)                  393728    ['bert[0][1]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 512)                  0         ['intermediate_layer[0][0]']  \n",
      "                                                                                                  \n",
      " output_layer (Dense)        (None, 10)                   5130      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108709130 (414.69 MB)\n",
      "Trainable params: 108709130 (414.69 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
    "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer', kernel_regularizer=regularizers.l2(0.01))(bert_embds)\n",
    "drop_out = tf.keras.layers.Dropout(0.2, name=\"dropout\")(intermediate_layer)\n",
    "output_layer = tf.keras.layers.Dense(10, activation='softmax', name='output_layer', kernel_regularizer=regularizers.l2(0.01))(drop_out) # softmax -> calcs probs of classes\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16caba49-9bf4-4921-beed-4784c2e6eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-5, decay_rate=1e-6, decay_steps=10000)\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "precision = tf.keras.metrics.Precision(name='precision')\n",
    "recall = tf.keras.metrics.Recall(name='recall')\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss_func, metrics=[acc, precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc286eb-9a58-4cf1-a1c8-9e4888e575ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      " 44/380 [==>...........................] - ETA: 45:29 - loss: 8.0207 - accuracy: 0.3707 - precision: 0.6571 - recall: 0.0327"
     ]
    }
   ],
   "source": [
    "hist =model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf9112-2421-4ed8-88bb-b9f83ecde6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"textclassification.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d3e7e6-4f2c-467f-aa88-438207c75a01",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23384b8d-f9b4-4548-8815-62d4fc8d4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041739e-2f22-45a8-b167-31a78ff49c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "\n",
    "# Plot the epoch vs accuracy graph\n",
    "plt.plot(range(1, len(train_acc) + 1), train_acc, label='Training Accuracy')\n",
    "plt.plot(range(1, len(val_acc) + 1), val_acc, label='Validation Accuracy')\n",
    "plt.title('Epoch vs Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss,accuracy,precision,recall=model.evaluate(val_dataset)\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"loss is : \", loss)\n",
    "\n",
    "print(\"accuracy is: \", str(round(accuracy*100))+ \"%\")\n",
    "print(\"recall is: \", str(round(recall*100)) + \"%\")\n",
    "print(\"Precision is : \", str(round(precision*100)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba1834-d0c7-40b0-95c2-4fcf890640d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c08d80-fe9b-4fd6-9b1f-54af0f296f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('model')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def prepare_data(input_text, tokenizer):\n",
    "    token = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=256, \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        add_special_tokens=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tf.cast(token.input_ids, tf.float64),\n",
    "        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n",
    "    }\n",
    "\n",
    "def make_prediction(model, processed_data, classes=['business', 'Entertainment', 'food', 'Graphichs', 'historical','medical','politcis', 'space','sport','technology']):\n",
    "    probs = model.predict(processed_data)[0]\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a445fcc-cea7-4dfc-bc5a-db16dfe12ef8",
   "metadata": {},
   "source": [
    "#  Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45bb2d2-1e82-41ca-b488-f04b88b88e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {0:\"Space\", 1:\"Politics\",2:\"Sport\",3:\"technology\",4:\"historical\", 5:\"Medical\", 6:\"Graphics\",7:\"Entertrainment\",8:\"Food\",9:\"business\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba376a1c-5042-4ab3-b445-4d44d277d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Bank voted 8-1 for no rate change\n",
    "\n",
    "The decision to keep interest rates on hold at 4.75% earlier this month was passed 8-1 by the Bank of England's rate-setting body, minutes have shown.\n",
    "\n",
    "One member of the Bank's Monetary Policy Committee (MPC) - Paul Tucker - voted to raise rates to 5%. The news surprised some analysts who had expected the latest minutes to show another unanimous decision. Worries over growth rates and consumer spending were behind the decision to freeze rates, the minutes showed. The Bank's latest inflation report, released last week, had noted that the main reason inflation might fall was weaker consumer spending.\n",
    "\n",
    "However, MPC member Paul Tucker voted for a quarter point rise in interest rates to 5%. He argued that economic growth was picking up, and that the equity, credit and housing markets had been stronger than expected.\n",
    "\n",
    "The Bank's minutes said that risks to the inflation forecast were \"sufficiently to the downside\" to keep rates on hold at its latest meeting. However, the minutes added: \"Some members noted that an increase might be warranted in due course if the economy evolved in line with the central projection\". Ross Walker, UK economist at Royal Bank of Scotland, said he was surprised that a dissenting vote had been made so soon. He said the minutes appeared to be \"trying to get the market to focus on the possibility of a rise in rates\". \"If the economy pans out as they expect then they are probably going to have to hike rates.\" However, he added, any rate increase is not likely to happen until later this year, with MPC members likely to look for a more sustainable pick up in consumer spending before acting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344283a-bb94-4ebc-a5d8-827a17bc1c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d6aa6-32da-47af-8a94-58cec959f29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29878d9-cfb0-4c40-963e-bfe7f55c8f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69a094-4ac1-4161-acbf-9dccbca84cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0c223-e177-4cc3-a8d2-79859f49bf27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7fd197-42bb-4873-8d6c-ecb8bb89ca04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70023fee-0b51-41e1-919d-a0fa37f5aa73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185de99-bd8d-4f42-a171-17473166486a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c333870-6b15-40e9-a2c0-fd3b62819e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d5baf-499e-4658-a40a-bd13d3047378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3e9ac-c243-4e63-8e19-684563c576c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
